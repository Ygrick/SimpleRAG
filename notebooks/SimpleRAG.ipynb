{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate==0.4.3\n",
    "# Остальные зависимости в requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from evaluate import load\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зададим константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Датасет с Hugging Face\n",
    "DATASET = \"neural-bridge/rag-dataset-1200\"\n",
    "SPLIT_DATASET = \"test\"\n",
    "\n",
    "# Модели\n",
    "    # Загружаем модель эмбедингов\n",
    "EMBEDDING_MODEL = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large\",\n",
    "    model_kwargs={\"device\": device}\n",
    ")\n",
    "\n",
    "# Определяем модель для генерации ответа на основе документов\n",
    "LLM_MODEL = \"meta-llama/llama-3.2-3b-instruct:free\"\n",
    "\n",
    "# API-конфигурация к LLM\n",
    "CLIENT = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\", \n",
    "    api_key=os.getenv(\"TOKEN_OPENAI\")\n",
    ")\n",
    "\n",
    "BERTSCORE = load(\"bertscore\")\n",
    "\n",
    "# Создадим два промпта для уменьшения вероятности нерелевантного ответа\n",
    "\n",
    "    # Промпт для LLM, который просит определить только релевантные документы\n",
    "DOC_RETRIEVAL_PROMPT = (\n",
    "    \"You are an AI assistant specialized in document retrieval. \"\n",
    "    \"Your task is to extract only the most relevant document IDs and chunk IDs from the provided documents. \"\n",
    "    \"Strictly follow these rules: \"\n",
    "    \"1. Return only a JSON object in this exact format: \"\n",
    "    '{\"relevant_documents\": [{\"document_id\": <doc_id>, \"chunk_id\": <chunk_id>}, ...]}. '\n",
    "    \"2. Do not modify, summarize, or explain the documents. \"\n",
    "    \"3. Do not include any additional text, explanations, reasoning, or commentary. \"\n",
    "    \"4. Do not return the document content, only IDs. \"\n",
    "    \"5. If no relevant documents exist, return an empty JSON: {\\\"relevant_documents\\\": []}. \"\n",
    "    \"6. Any deviation from these rules is strictly prohibited.\"\n",
    ")\n",
    "\n",
    "    # Промпт для LLM, который просит составить ответ только на релевантных документах\n",
    "ANSWER_GENERATION_PROMPT = (\n",
    "    \"You are an assistant that answers user questions based strictly on the provided documents. \"\n",
    "    \"Use only the content from the relevant documents and chunks listed below: \"\n",
    "    \"{retrieved_data} \"\n",
    "    \"Now, generate a well-structured answer to the user's question.\"\n",
    "    \"Do not make up information. If the answer is unclear from the documents, say 'Insufficient information'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определим данные и создадим базу знаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'question', 'answer'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': 'Trail Patrol Training\\nWant to be a part of the Trail Patrol ?? Join an Orientation & Hike on the 1st Tuesday of each month. This course is required for all PATC members interested in joining the PATC Trail Patrol.\\nThe course teaches the essential skills necessary to be a trail patrol member and to provide a reassuring presence on the trail while teaching safety and environmental responsibility. A Trail Patrol handbook is provided to all students. Please bring a pencil, your hiking daypack & lunch.\\nMore Info: View the Calendar or contact TP Training or visit the Trail Patrol Training web pages.\\nHike Leader Class.\\nMore Info: Contact Hike Leader Training or click here to register.\\nBackpacking Classes\\nEducating people in safe and environmentally friendly practices for traveling into the backcountry is one of Trail Patrol’s core responsibilities. We offer backpacking classes for novices seeking to take up backpacking as well as for experienced backpackers.\\nBackpacking 101: An Introductory Course is for beginners & those who want to update rusty skills. It includes a weekend overnight backpacking trip. Instruction covers equipment selection and use, information and techniques to enhance safety and comfort, and Leave No Trace methods to protect our fragile backcountry environment.\\nMore Info: Contact Backpacking Training or click here to register.\\nBackpacking 202: Planning and leading group trips is for Backpacking 101 graduates and others with comparable experience. This class provides the opportunity to take it to the next level by developing special skill needed to plan and lead group trips. Pre-trip planning sessions will cover equipment selection and use, route planning, food planning, improving safety and comfort, managing the unexpected, and Leave No Trace principles to protect our fragile backcountry environment.\\nIncludes a weekend overnight backpacking trip.\\nMore Info: Contact Backpacking Training or click here to register.\\nLightweight Backpacking: Techniques for reducing pack weight without compromising safety - for both experienced and new backpackers. One-day workshop, no backpacking trip. This one-day workshop is designed to acquaint backpackers with the importance of weight considerations when choosing and using equipment, and to instill a weight-conscience mind set when packing for an overnight trip. It is not intended to advocate an ultra light philosophy. Each student attending the class will be encouraged to bring a pack fully loaded, less food and water, with everything they would normally carry on a 3-day, 2-night trip of 10\\nPre-registration is required. Class is limited to 25 students.\\nMore Info: Contact Backpacking Training or click here to register.\\nWilderness First Aid\\nBasic Wilderness First Aid (BWFA) is a 2-day workshop. Day one covers Adult CPR and AED and American Heart Association First Aid. You will receive a textbook and a certification card good for two years. Day two is American Safety and Health Institute (ASHI) Basic Wilderness First Aid. You will learn how to do bleeding control, splinting and other basic first aid skills in the wilderness setting. There is plenty of hands-on time & paramedics with years of backcountry experience teach the classes.\\nMore Info: Contact TP First Aid or click here to register.\\nWilderness First Aid (WFA)\\nJoin us for a 20-hour Wilderness First Aid Class! In this class, you will learn how to get help, move and transport patients, conduct patient assessments, documentation, how to handle medical and environmental emergencies, injury prevention and care, and much more. There is plenty of hands-on practice time as well as scenarios. This class is conducted inside and outside on the trail. Each student will receive a certification card good for two years through ECSI and a waterproof field guide. No experience required.\\nMore Info: Contact TP First Aid or click here to register.\\nLeave No Trace Trainer Course\\nThese courses are designed to enhance your understanding of Leave No Trace practices and ethics and to increase your level of expertise and confidence in teaching Leave No Trace skills. Through focused activities, hands‑on field experience and both formal and informal discussions, you will be introduced to concepts and methods that will advance your knowledge of Leave No Trace issues, expand your repertoire of low‑impact skills and increase your effectiveness in teaching these important skills to others.\\nOn completion of this course, participants will be registered as Leave No Trace Trainers with the national Leave No Trace Center for Outdoor Ethics and will receive a certificate of course completion as well as their Leave No Trace Trainer lapel pin.\\nMore Info: Contact TP LNT or click here to register.',\n",
       " 'question': 'What are some of the skills taught in the Trail Patrol Training course?',\n",
       " 'answer': 'The course teaches the essential skills necessary to be a trail patrol member and to provide a reassuring presence on the trail while teaching safety and environmental responsibility.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем данные и смотрим что в них\n",
    "rag_dataset = load_dataset(DATASET, split=SPLIT_DATASET)\n",
    "display(rag_dataset)\n",
    "\n",
    "# Смотрим пример одного row\n",
    "print('Dataset example:') \n",
    "rag_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents: List[str], chunk_size: int = 1000, chunk_overlap: int = 100) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Разбивает документы на чанки.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): Список документов.\n",
    "        chunk_size (int): Размер чанка в символах.\n",
    "        chunk_overlap (int): Перекрытие чанков.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Разбитые на чанки документы.\n",
    "    \"\"\"\n",
    "    print(f\"Разбиваем {len(documents)} документов на чанки (размер: {chunk_size}, перекрытие: {chunk_overlap})\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_documents = []\n",
    "\n",
    "    # Предобработка всех текстовых файлов\\документов\\текстов\n",
    "    for i, document in enumerate(documents):\n",
    "        # Минимально очистим текст\n",
    "        text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', document)  # Удаляем лишние пустые строки\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # Заменяем табуляции на пробелы\n",
    "        text = text.strip()\n",
    "\n",
    "        # Создадим langchain-документ (совместимый формат для RecursiveCharacterTextSplitter)\n",
    "        langchain_document = Document(page_content=text)\n",
    "        # Разобьём длинный текст на чанки\n",
    "        chunks = text_splitter.split_documents([langchain_document])\n",
    "\n",
    "        # Добавим номер документа (текста) и номер чанка как доп.информацию\n",
    "        # И добавим сам чанк в список всех чанков\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"document_id\"] = i + 1\n",
    "            chunk.metadata[\"chunk_id\"] = j + 1\n",
    "            chunked_documents.append(chunk)\n",
    "\n",
    "    print(f\"Создано {len(chunked_documents)} чанков\")\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(documents: List[Document], weights: List[float] = [0.4, 0.6]) -> EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Создаёт EnsembleRetriever на основе FAISS и BM25.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): Список документов для индексации.\n",
    "        weights (List[float]): Веса для каждого из retrievers\n",
    "\n",
    "    Returns:\n",
    "        EnsembleRetriever: Комбинированный ретривер для поиска.\n",
    "    \"\"\"\n",
    "    # FAISS retriever\n",
    "    vector_store = FAISS.from_documents(documents, EMBEDDING_MODEL)\n",
    "    faiss_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={'k': 2}  # Количество возвращаемых документов\n",
    "    )\n",
    "\n",
    "    # BM25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 2  # Количество возвращаемых документов\n",
    "\n",
    "    # Объединяем их в EnsembleRetriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, faiss_retriever],\n",
    "        weights=weights  # Вес каждого retriever\n",
    "    )\n",
    "\n",
    "    return ensemble_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разбиваем 240 документов на чанки (размер: 1000, перекрытие: 100)\n",
      "Создано 1139 чанков\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'document_id': 1, 'chunk_id': 4}, page_content='More Info: Contact Backpacking Training or click here to register.\\nWilderness First Aid\\nBasic Wilderness First Aid (BWFA) is a 2-day workshop. Day one covers Adult CPR and AED and American Heart Association First Aid. You will receive a textbook and a certification card good for two years. Day two is American Safety and Health Institute (ASHI) Basic Wilderness First Aid. You will learn how to do bleeding control, splinting and other basic first aid skills in the wilderness setting. There is plenty of hands-on time & paramedics with years of backcountry experience teach the classes.\\nMore Info: Contact TP First Aid or click here to register.\\nWilderness First Aid (WFA)')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents = chunk_documents(rag_dataset['context'])\n",
    "chunked_documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = create_retriever(chunked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Убедимся что всё работает на тестовом примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who were the two convicted killers that escaped from an upstate New York maximum-security prison?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример запроса\n",
    "test_query = rag_dataset['question'][3]\n",
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(system_prompt: str, docs: str, query: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Отправляет запрос в LLM, используя заданный системный промпт.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): Промпт, определяющий задачу для модели.\n",
    "        docs (str): Документы в формате JSON, содержащие релевантную информацию.\n",
    "        query (str): Вопрос пользователя.\n",
    "        temperature (float): Температура ответа (насколько модель креативна)\n",
    "\n",
    "    Returns:\n",
    "        str: Ответ LLM.\n",
    "    \"\"\"\n",
    "    # Формируем контекст для LLM\n",
    "    chat_history = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'documents', 'content': docs},\n",
    "        {'role': 'user', 'content': query}\n",
    "    ]\n",
    "\n",
    "\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=chat_history,\n",
    "        temperature=temperature,\n",
    "        max_tokens=2048\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск релевантных документов\n",
    "relevant_docs = ensemble_retriever.invoke(test_query)\n",
    "\n",
    "# Преобразуем найденные документы в нужный нам формат\n",
    "relevant_docs_data = [\n",
    "    {\n",
    "        \"document_id\": doc.metadata.get(\"document_id\", -1),\n",
    "        \"chunk_id\": doc.metadata.get(\"chunk_id\", -1),\n",
    "        \"content\": doc.page_content\n",
    "    }\n",
    "    for doc in relevant_docs\n",
    "]\n",
    "json_docs = json.dumps(relevant_docs_data, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полученные идентификаторы документов: {\"relevant_documents\": [{\"document_id\": 4, \"chunk_id\": 1}]}\n"
     ]
    }
   ],
   "source": [
    "# Первый запрос к LLM: получение списка ID релевантных документов\n",
    "id_docs_response = get_llm_response(\n",
    "    system_prompt=DOC_RETRIEVAL_PROMPT, \n",
    "    docs=json_docs, \n",
    "    query=test_query,\n",
    "    temperature=0.0 # нам нужен наиболее строгий ответ\n",
    ")\n",
    "print(f\"Полученные идентификаторы документов: {id_docs_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two convicted killers who escaped from an upstate New York maximum-security prison were Richard Matt and David Sweat.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Второй запрос к LLM: генерация финального ответа\n",
    "response = get_llm_response(\n",
    "    system_prompt=ANSWER_GENERATION_PROMPT.format(retrieved_data=id_docs_response), \n",
    "    docs=json_docs, \n",
    "    query=test_query,\n",
    "    temperature=0.3 # можем быть слегка креативными\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two convicted killers that escaped from an upstate New York maximum-security prison were Richard Matt and David Sweat.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset[3]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим насколько качественно работает наша RAG система"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What are some of the features of the yellow pa...</td>\n",
       "      <td>The yellow paper plates mentioned in the conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is a \"Cultural Muslim\" according to Kaigh...</td>\n",
       "      <td>A \"Cultural Muslim\" is someone who calls thems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Who are the main characters in the book \"Odd a...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What are some features of the WHIRLPOOL BATHTU...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>What is the rank of Iasi in terms of populatio...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "24   What are some of the features of the yellow pa...   \n",
       "6    What is a \"Cultural Muslim\" according to Kaigh...   \n",
       "93   Who are the main characters in the book \"Odd a...   \n",
       "109  What are some features of the WHIRLPOOL BATHTU...   \n",
       "104  What is the rank of Iasi in terms of populatio...   \n",
       "\n",
       "                                                answer  \n",
       "24   The yellow paper plates mentioned in the conte...  \n",
       "6    A \"Cultural Muslim\" is someone who calls thems...  \n",
       "93   The main characters in the book \"Odd and the F...  \n",
       "109  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...  \n",
       "104  Iasi is the 4th biggest city in Romania in ter...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Положим вопросы и ответы в датафрейм и возьмём лишь 20 примеров (т.к. API имеет лимиты)\n",
    "df = pd.DataFrame({\n",
    "    'question': rag_dataset['question'],\n",
    "    'answer': rag_dataset['answer'],\n",
    "})\n",
    "df_sample = df.sample(20, random_state=42)\n",
    "df_sample.reset_index(drop=True, inplace=True)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query):\n",
    "    \"\"\"\n",
    "    Генерирует ответ на заданный запрос, используя retriever, документы и LLM.\n",
    "\n",
    "    Аргументы:\n",
    "        query (str): Вопрос пользователя.\n",
    "\n",
    "    Возвращает:\n",
    "        str: Сгенерированный ответ модели.\n",
    "    \"\"\"\n",
    "    # Поиск релевантных документов\n",
    "    relevant_docs = ensemble_retriever.invoke(query)\n",
    "    relevant_docs_data = [\n",
    "        {\n",
    "            \"document_id\": doc.metadata.get(\"document_id\", -1),\n",
    "            \"chunk_id\": doc.metadata.get(\"chunk_id\", -1),\n",
    "            \"content\": doc.page_content\n",
    "        }\n",
    "        for doc in relevant_docs\n",
    "    ]\n",
    "    json_docs = json.dumps(relevant_docs_data, ensure_ascii=False)\n",
    "    \n",
    "    # Обращения к LLM\n",
    "    id_docs_response = get_llm_response(\n",
    "        system_prompt=DOC_RETRIEVAL_PROMPT, \n",
    "        docs=json_docs, \n",
    "        query=query,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    response = get_llm_response(\n",
    "        system_prompt=ANSWER_GENERATION_PROMPT.format(retrieved_data=id_docs_response), \n",
    "        docs=json_docs, \n",
    "        query=query,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Нужно из-за ограничений API к LLM\n",
    "    time.sleep(17)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>rag_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What are some of the features of the yellow pa...</td>\n",
       "      <td>The yellow paper plates mentioned in the conte...</td>\n",
       "      <td>Based on the provided documents, some of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is a \"Cultural Muslim\" according to Kaigh...</td>\n",
       "      <td>A \"Cultural Muslim\" is someone who calls thems...</td>\n",
       "      <td>According to the documents, a \"Cultural Muslim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Who are the main characters in the book \"Odd a...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "      <td>Insufficient information. The provided documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What are some features of the WHIRLPOOL BATHTU...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...</td>\n",
       "      <td>According to the provided documents, the WHIRL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>What is the rank of Iasi in terms of populatio...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "      <td>According to the documents, Iasi is the 4th bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "24   What are some of the features of the yellow pa...   \n",
       "6    What is a \"Cultural Muslim\" according to Kaigh...   \n",
       "93   Who are the main characters in the book \"Odd a...   \n",
       "109  What are some features of the WHIRLPOOL BATHTU...   \n",
       "104  What is the rank of Iasi in terms of populatio...   \n",
       "\n",
       "                                                answer  \\\n",
       "24   The yellow paper plates mentioned in the conte...   \n",
       "6    A \"Cultural Muslim\" is someone who calls thems...   \n",
       "93   The main characters in the book \"Odd and the F...   \n",
       "109  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...   \n",
       "104  Iasi is the 4th biggest city in Romania in ter...   \n",
       "\n",
       "                                            rag_answer  \n",
       "24   Based on the provided documents, some of the f...  \n",
       "6    According to the documents, a \"Cultural Muslim...  \n",
       "93   Insufficient information. The provided documen...  \n",
       "109  According to the provided documents, the WHIRL...  \n",
       "104  According to the documents, Iasi is the 4th bi...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['rag_answer'] = df_sample['question'].apply(rag_answer)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(predictions: List[str], references: List[str], lang: str = \"en\") -> float:\n",
    "    \"\"\"\n",
    "    Вычисляет средний F1-скор BertScore между предсказанными и эталонными ответами.\n",
    "\n",
    "    Аргументы:\n",
    "        predictions (List[str]): Список сгенерированных моделью ответов.\n",
    "        references (List[str]): Список эталонных (правильных) ответов.\n",
    "        lang (str): Язык текстов (по умолчанию \"en\").\n",
    "\n",
    "    Возвращает:\n",
    "        float: Средний F1 из BertScore.\n",
    "    \"\"\"\n",
    "    results = BERTSCORE.compute(predictions=predictions, references=references, lang=lang)\n",
    "    return float(np.mean(results[\"f1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore F1: 0.9278\n"
     ]
    }
   ],
   "source": [
    "avg_f1 = compute_bertscore(df_sample['rag_answer'].tolist(), df_sample['answer'].tolist())\n",
    "print(f\"BertScore F1: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
