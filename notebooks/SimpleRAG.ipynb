{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate==0.4.3 bert-score==0.3.13\n",
    "# –û—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from evaluate import load\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–¥–∞–¥–∏–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert-score –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫–∏\n",
    "BERTSCORE = load(\"bertscore\")\n",
    "\n",
    "# –î–∞—Ç–∞—Å–µ—Ç —Å Hugging Face\n",
    "DATASET = \"neural-bridge/rag-dataset-1200\"\n",
    "SPLIT_DATASET = \"test\"\n",
    "\n",
    "# –ú–æ–¥–µ–ª–∏\n",
    "## –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–∏–Ω–≥–æ–≤\n",
    "EMBEDDING_MODEL = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large\",\n",
    "    model_kwargs={\"device\": device}\n",
    ")\n",
    "\n",
    "# –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ API –∫ LLM –æ—Ç —Å–µ—Ä–≤–∏—Å–∞ https://openrouter.ai/api/v1\n",
    "## –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "# LLM_MODEL = \"meta-llama/llama-3.2-3b-instruct:free\"\n",
    "## API-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫ LLM\n",
    "# CLIENT = OpenAI(\n",
    "#     base_url=\"https://openrouter.ai/api/v1\", \n",
    "#     api_key=os.getenv(\"TOKEN_OPENAI\")\n",
    "# )\n",
    "\n",
    "# –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ API –∫ LLM –æ—Ç —Å–µ—Ä–≤–∏—Å–∞ https://api.together.xyz/v1\n",
    "## –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "LLM_MODEL = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "## API-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫ LLM\n",
    "CLIENT = OpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\", \n",
    "    api_key=os.getenv(\"TOKEN_TAI\")\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–¥–∏–º –¥–≤–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    "\n",
    "## –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ—Å–∏—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
    "DOC_RETRIEVAL_PROMPT = (\n",
    "    \"You are an AI assistant specialized in document retrieval. \"\n",
    "    \"Your task is to extract only the most relevant document IDs and chunk IDs from the provided documents. \"\n",
    "    \"Strictly follow these rules: \"\n",
    "    \"1. Return only a JSON object in this exact format: \"\n",
    "    '{\"relevant_documents\": [{\"document_id\": <doc_id>, \"chunk_id\": <chunk_id>}, ...]}. '\n",
    "    \"2. Do not modify, summarize, or explain the documents. \"\n",
    "    \"3. Do not include any additional text, explanations, reasoning, or commentary. \"\n",
    "    \"4. Do not return the document content, only IDs. \"\n",
    "    \"5. If no relevant documents exist, return an empty JSON: {\\\"relevant_documents\\\": []}. \"\n",
    "    \"6. Any deviation from these rules is strictly prohibited.\"\n",
    ")\n",
    "\n",
    "## –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ—Å–∏—Ç —Å–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–≤–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "ANSWER_GENERATION_PROMPT = (\n",
    "    \"You are an assistant that answers user questions based strictly on the provided documents. \"\n",
    "    \"Use only the content from the relevant documents and chunks listed below: \"\n",
    "    \"{retrieved_data} \"\n",
    "    \"Now, generate a well-structured answer to the user's question.\"\n",
    "    \"Do not make up information. If the answer is unclear from the documents, say 'Insufficient information'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–ø—Ä–µ–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞–¥–∏–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'question', 'answer'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': 'Trail Patrol Training\\nWant to be a part of the Trail Patrol ?? Join an Orientation & Hike on the 1st Tuesday of each month. This course is required for all PATC members interested in joining the PATC Trail Patrol.\\nThe course teaches the essential skills necessary to be a trail patrol member and to provide a reassuring presence on the trail while teaching safety and environmental responsibility. A Trail Patrol handbook is provided to all students. Please bring a pencil, your hiking daypack & lunch.\\nMore Info: View the Calendar or contact TP Training or visit the Trail Patrol Training web pages.\\nHike Leader Class.\\nMore Info: Contact Hike Leader Training or click here to register.\\nBackpacking Classes\\nEducating people in safe and environmentally friendly practices for traveling into the backcountry is one of Trail Patrol‚Äôs core responsibilities. We offer backpacking classes for novices seeking to take up backpacking as well as for experienced backpackers.\\nBackpacking 101: An Introductory Course is for beginners & those who want to update rusty skills. It includes a weekend overnight backpacking trip. Instruction covers equipment selection and use, information and techniques to enhance safety and comfort, and Leave No Trace methods to protect our fragile backcountry environment.\\nMore Info: Contact Backpacking Training or click here to register.\\nBackpacking 202: Planning and leading group trips is for Backpacking 101 graduates and others with comparable experience. This class provides the opportunity to take it to the next level by developing special skill needed to plan and lead group trips. Pre-trip planning sessions will cover equipment selection and use, route planning, food planning, improving safety and comfort, managing the unexpected, and Leave No Trace principles to protect our fragile backcountry environment.\\nIncludes a weekend overnight backpacking trip.\\nMore Info: Contact Backpacking Training or click here to register.\\nLightweight Backpacking: Techniques for reducing pack weight without compromising safety - for both experienced and new backpackers. One-day workshop, no backpacking trip. This one-day workshop is designed to acquaint backpackers with the importance of weight considerations when choosing and using equipment, and to instill a weight-conscience mind set when packing for an overnight trip. It is not intended to advocate an ultra light philosophy. Each student attending the class will be encouraged to bring a pack fully loaded, less food and water, with everything they would normally carry on a 3-day, 2-night trip of 10\\nPre-registration is required. Class is limited to 25 students.\\nMore Info: Contact Backpacking Training or click here to register.\\nWilderness First Aid\\nBasic Wilderness First Aid (BWFA) is a 2-day workshop. Day one covers Adult CPR and AED and American Heart Association First Aid. You will receive a textbook and a certification card good for two years. Day two is American Safety and Health Institute (ASHI) Basic Wilderness First Aid. You will learn how to do bleeding control, splinting and other basic first aid skills in the wilderness setting. There is plenty of hands-on time & paramedics with years of backcountry experience teach the classes.\\nMore Info: Contact TP First Aid or click here to register.\\nWilderness First Aid (WFA)\\nJoin us for a 20-hour Wilderness First Aid Class! In this class, you will learn how to get help, move and transport patients, conduct patient assessments, documentation, how to handle medical and environmental emergencies, injury prevention and care, and much more. There is plenty of hands-on practice time as well as scenarios. This class is conducted inside and outside on the trail. Each student will receive a certification card good for two years through ECSI and a waterproof field guide. No experience required.\\nMore Info: Contact TP First Aid or click here to register.\\nLeave No Trace Trainer Course\\nThese courses are designed to enhance your understanding of Leave No Trace practices and ethics and to increase your level of expertise and confidence in teaching Leave No Trace skills. Through focused activities, hands‚Äëon field experience and both formal and informal discussions, you will be introduced to concepts and methods that will advance your knowledge of Leave No Trace issues, expand your repertoire of low‚Äëimpact skills and increase your effectiveness in teaching these important skills to others.\\nOn completion of this course, participants will be registered as Leave No Trace Trainers with the national Leave No Trace Center for Outdoor Ethics and will receive a certificate of course completion as well as their Leave No Trace Trainer lapel pin.\\nMore Info: Contact TP LNT or click here to register.',\n",
       " 'question': 'What are some of the skills taught in the Trail Patrol Training course?',\n",
       " 'answer': 'The course teaches the essential skills necessary to be a trail patrol member and to provide a reassuring presence on the trail while teaching safety and environmental responsibility.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ —Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –≤ –Ω–∏—Ö\n",
    "rag_dataset = load_dataset(DATASET, split=SPLIT_DATASET)\n",
    "display(rag_dataset)\n",
    "\n",
    "# –°–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä –æ–¥–Ω–æ–≥–æ row\n",
    "print('Dataset example:') \n",
    "rag_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents: List[str], chunk_size: int = 1000, chunk_overlap: int = 100) -> List[Document]:\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "        chunk_size (int): –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö.\n",
    "        chunk_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: –†–∞–∑–±–∏—Ç—ã–µ –Ω–∞ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã.\n",
    "    \"\"\"\n",
    "    print(f\"–†–∞–∑–±–∏–≤–∞–µ–º {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä: {chunk_size}, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {chunk_overlap})\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_documents = []\n",
    "\n",
    "    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤\\–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\\—Ç–µ–∫—Å—Ç–æ–≤\n",
    "    for i, document in enumerate(documents):\n",
    "        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ –æ—á–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç\n",
    "        text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', document)  # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # –ó–∞–º–µ–Ω—è–µ–º —Ç–∞–±—É–ª—è—Ü–∏–∏ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã\n",
    "        text = text.strip()\n",
    "\n",
    "        # –°–æ–∑–¥–∞–¥–∏–º langchain-–¥–æ–∫—É–º–µ–Ω—Ç (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è RecursiveCharacterTextSplitter)\n",
    "        langchain_document = Document(page_content=text)\n",
    "        # –†–∞–∑–æ–±—å—ë–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏\n",
    "        chunks = text_splitter.split_documents([langchain_document])\n",
    "\n",
    "        # –î–æ–±–∞–≤–∏–º –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ç–µ–∫—Å—Ç–∞) –∏ –Ω–æ–º–µ—Ä —á–∞–Ω–∫–∞ –∫–∞–∫ –¥–æ–ø.–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "        # –ò –¥–æ–±–∞–≤–∏–º —Å–∞–º —á–∞–Ω–∫ –≤ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"document_id\"] = i + 1\n",
    "            chunk.metadata[\"chunk_id\"] = j + 1\n",
    "            chunked_documents.append(chunk)\n",
    "\n",
    "    print(f\"–°–æ–∑–¥–∞–Ω–æ {len(chunked_documents)} —á–∞–Ω–∫–æ–≤\")\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(documents: List[Document], weights: List[float] = [0.4, 0.6]) -> EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞—ë—Ç EnsembleRetriever –Ω–∞ –æ—Å–Ω–æ–≤–µ FAISS –∏ BM25.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.\n",
    "        weights (List[float]): –í–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ retrievers\n",
    "\n",
    "    Returns:\n",
    "        EnsembleRetriever: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞.\n",
    "    \"\"\"\n",
    "    # FAISS retriever (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º)\n",
    "    vector_store = FAISS.from_documents(documents, EMBEDDING_MODEL)\n",
    "    faiss_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={'k': 2}  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    )\n",
    "\n",
    "    # BM25 retriever (—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫) \n",
    "    # *–í –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º –¥–æ–º–µ–Ω–∞. \n",
    "    # *–î–ª—è –¥–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–ª–∏—à–µ—Å—Ç–≤–æ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞.\n",
    "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "    bm25_retriever.k = 2  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –≤ EnsembleRetriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, faiss_retriever],\n",
    "        weights=weights  # –í–µ—Å –∫–∞–∂–¥–æ–≥–æ retriever\n",
    "    )\n",
    "\n",
    "    return ensemble_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–±–∏–≤–∞–µ–º 240 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä: 1000, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: 100)\n",
      "–°–æ–∑–¥–∞–Ω–æ 1139 —á–∞–Ω–∫–æ–≤\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'document_id': 1, 'chunk_id': 4}, page_content='More Info: Contact Backpacking Training or click here to register.\\nWilderness First Aid\\nBasic Wilderness First Aid (BWFA) is a 2-day workshop. Day one covers Adult CPR and AED and American Heart Association First Aid. You will receive a textbook and a certification card good for two years. Day two is American Safety and Health Institute (ASHI) Basic Wilderness First Aid. You will learn how to do bleeding control, splinting and other basic first aid skills in the wilderness setting. There is plenty of hands-on time & paramedics with years of backcountry experience teach the classes.\\nMore Info: Contact TP First Aid or click here to register.\\nWilderness First Aid (WFA)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents = chunk_documents(rag_dataset['context'])\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\")\n",
    "chunked_documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = create_retriever(chunked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: Who were the two convicted killers that escaped from an upstate New York maximum-security prison?\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n",
    "test_query = rag_dataset['question'][3]\n",
    "print(\"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:\", test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(system_prompt: str, docs: str, query: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ LLM, –∏—Å–ø–æ–ª—å–∑—É—è –∑–∞–¥–∞–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): –ü—Ä–æ–º–ø—Ç, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π –∑–∞–¥–∞—á—É –¥–ª—è –º–æ–¥–µ–ª–∏.\n",
    "        docs (str): –î–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.\n",
    "        query (str): –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
    "        temperature (float): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ (–Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–∞)\n",
    "\n",
    "    Returns:\n",
    "        str: –û—Ç–≤–µ—Ç LLM.\n",
    "    \"\"\"\n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM\n",
    "    chat_history = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'documents', 'content': docs},\n",
    "        {'role': 'user', 'content': query}\n",
    "    ]\n",
    "\n",
    "    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=chat_history,\n",
    "        temperature=temperature,\n",
    "        max_tokens=2048\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
    "relevant_docs = ensemble_retriever.invoke(test_query)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –Ω—É–∂–Ω—ã–π –Ω–∞–º —Ñ–æ—Ä–º–∞—Ç\n",
    "relevant_docs_data = [\n",
    "    {\n",
    "        \"document_id\": doc.metadata.get(\"document_id\", -1),\n",
    "        \"chunk_id\": doc.metadata.get(\"chunk_id\", -1),\n",
    "        \"content\": doc.page_content\n",
    "    }\n",
    "    for doc in relevant_docs\n",
    "]\n",
    "json_docs = json.dumps(relevant_docs_data, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {\"relevant_documents\": [{\"document_id\": 4, \"chunk_id\": 1}]\n"
     ]
    }
   ],
   "source": [
    "# –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM: –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ ID —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "id_docs_response = get_llm_response(\n",
    "    system_prompt=DOC_RETRIEVAL_PROMPT, \n",
    "    docs=json_docs, \n",
    "    query=test_query,\n",
    "    temperature=0.0 # –ñ—ë—Å—Ç–∫–∏–π —Ä–µ–∂–∏–º –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    ")\n",
    "print(f\"–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {id_docs_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: The two convicted killers who escaped from an upstate New York maximum-security prison were Richard Matt and David Sweat. Richard Matt was serving 25 years to life for killing and dismembering his former boss, while David Sweat was serving a sentence of life without parole for killing a sheriff's deputy in Broome County in 2002.\n",
      "\n",
      "–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: The two convicted killers that escaped from an upstate New York maximum-security prison were Richard Matt and David Sweat.\n"
     ]
    }
   ],
   "source": [
    "# –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å –∫ LLM: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    "response = get_llm_response(\n",
    "    system_prompt=ANSWER_GENERATION_PROMPT.format(retrieved_data=id_docs_response), \n",
    "    docs=json_docs, \n",
    "    query=test_query,\n",
    "    temperature=0.3 # –†–∞–∑—Ä–µ—à–∞–µ–º –±—ã—Ç—å —Å–ª–µ–≥–∫–∞ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–º–∏\n",
    ")\n",
    "print(\"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", response)\n",
    "print(\"\\n–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\", rag_dataset[3]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞—à–∞ RAG —Å–∏—Å—Ç–µ–º–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some of the features of the yellow pa...</td>\n",
       "      <td>The yellow paper plates mentioned in the conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a \"Cultural Muslim\" according to Kaigh...</td>\n",
       "      <td>A \"Cultural Muslim\" is someone who calls thems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the main characters in the book \"Odd a...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some features of the WHIRLPOOL BATHTU...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the rank of Iasi in terms of populatio...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are some of the features of the yellow pa...   \n",
       "1  What is a \"Cultural Muslim\" according to Kaigh...   \n",
       "2  Who are the main characters in the book \"Odd a...   \n",
       "3  What are some features of the WHIRLPOOL BATHTU...   \n",
       "4  What is the rank of Iasi in terms of populatio...   \n",
       "\n",
       "                                              answer  \n",
       "0  The yellow paper plates mentioned in the conte...  \n",
       "1  A \"Cultural Muslim\" is someone who calls thems...  \n",
       "2  The main characters in the book \"Odd and the F...  \n",
       "3  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...  \n",
       "4  Iasi is the 4th biggest city in Romania in ter...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ü–æ–ª–æ–∂–∏–º –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∏ –≤–æ–∑—å–º—ë–º –ª–∏—à—å 20 –ø—Ä–∏–º–µ—Ä–æ–≤ (—Ç.–∫. API –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç—ã)\n",
    "df = pd.DataFrame({\n",
    "    'question': rag_dataset['question'],\n",
    "    'answer': rag_dataset['answer'],\n",
    "})\n",
    "df_sample = df.sample(100, random_state=42)\n",
    "df_sample.reset_index(drop=True, inplace=True)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query: str) -> str:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è retriever, –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ LLM.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        query (str): –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏.\n",
    "    \"\"\"\n",
    "    # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    relevant_docs = ensemble_retriever.invoke(query)\n",
    "    relevant_docs_data = [\n",
    "        {\n",
    "            \"document_id\": doc.metadata.get(\"document_id\", -1),\n",
    "            \"chunk_id\": doc.metadata.get(\"chunk_id\", -1),\n",
    "            \"content\": doc.page_content\n",
    "        }\n",
    "        for doc in relevant_docs\n",
    "    ]\n",
    "    json_docs = json.dumps(relevant_docs_data, ensure_ascii=False)\n",
    "    \n",
    "    # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: –ø–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    id_docs_response = get_llm_response(\n",
    "        system_prompt=DOC_RETRIEVAL_PROMPT, \n",
    "        docs=json_docs, \n",
    "        query=query,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤ API\n",
    "    time.sleep(10)\n",
    "    # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    response = get_llm_response(\n",
    "        system_prompt=ANSWER_GENERATION_PROMPT.format(retrieved_data=id_docs_response), \n",
    "        docs=json_docs, \n",
    "        query=query,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤ API\n",
    "    time.sleep(10)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>rag_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some of the features of the yellow pa...</td>\n",
       "      <td>The yellow paper plates mentioned in the conte...</td>\n",
       "      <td>The features of the yellow paper plates mentio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a \"Cultural Muslim\" according to Kaigh...</td>\n",
       "      <td>A \"Cultural Muslim\" is someone who calls thems...</td>\n",
       "      <td>According to Kaighla Um Dayo, a!Cultural Musli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the main characters in the book \"Odd a...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some features of the WHIRLPOOL BATHTU...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the rank of Iasi in terms of populatio...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are some of the features of the yellow pa...   \n",
       "1  What is a \"Cultural Muslim\" according to Kaigh...   \n",
       "2  Who are the main characters in the book \"Odd a...   \n",
       "3  What are some features of the WHIRLPOOL BATHTU...   \n",
       "4  What is the rank of Iasi in terms of populatio...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The yellow paper plates mentioned in the conte...   \n",
       "1  A \"Cultural Muslim\" is someone who calls thems...   \n",
       "2  The main characters in the book \"Odd and the F...   \n",
       "3  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...   \n",
       "4  Iasi is the 4th biggest city in Romania in ter...   \n",
       "\n",
       "                                          rag_answer  \n",
       "0  The features of the yellow paper plates mentio...  \n",
       "1  According to Kaighla Um Dayo, a!Cultural Musli...  \n",
       "2  The main characters in the book \"Odd and the F...  \n",
       "3  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has t...  \n",
       "4  Iasi is the 4th biggest city in Romania in ter...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['rag_answer'] = df_sample['question'].apply(rag_answer)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(predictions: List[str], references: List[str], lang: str = \"en\") -> float:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π F1-—Å–∫–æ—Ä BertScore –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        predictions (List[str]): –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "        references (List[str]): –°–ø–∏—Å–æ–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö (–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö) –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "        lang (str): –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é \"en\").\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        float: –°—Ä–µ–¥–Ω–∏–π F1 –∏–∑ BertScore.\n",
    "    \"\"\"\n",
    "    results = BERTSCORE.compute(predictions=predictions, references=references, lang=lang)\n",
    "    return float(np.mean(results[\"f1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore F1 (—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏): 0.9312\n"
     ]
    }
   ],
   "source": [
    "avg_f1 = compute_bertscore(df_sample['rag_answer'].tolist(), df_sample['answer'].tolist())\n",
    "print(f\"BertScore F1 (—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏): {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–≤–µ—Ä–∏–º –±—É–¥–µ—Ç –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ª—É—á—à–µ, –µ—Å–ª–∏ –º—ã —É–±–µ—Ä–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_v2(query):\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è retriever, –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ LLM.\n",
    "    –û—Ç–ª–∏—á–∏–µ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        query (str): –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏.\n",
    "    \"\"\"\n",
    "    # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    relevant_docs = ensemble_retriever.invoke(query)\n",
    "    relevant_docs_data = [\n",
    "        {\n",
    "            \"document_id\": doc.metadata.get(\"document_id\", -1),\n",
    "            \"chunk_id\": doc.metadata.get(\"chunk_id\", -1),\n",
    "            \"content\": doc.page_content\n",
    "        }\n",
    "        for doc in relevant_docs\n",
    "    ]\n",
    "    json_docs = json.dumps(relevant_docs_data, ensure_ascii=False)\n",
    "    \n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –±–µ–∑ —à–∞–≥–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that answers user questions based strictly on the provided documents. \"\n",
    "        \"Use only the content from the relevant documents.\"\n",
    "        \"Now, generate a well-structured answer to the user's question.\"\n",
    "        \"Do not make up information. If the answer is unclear from the documents, say 'Insufficient information'.\"\n",
    "    )\n",
    "    response = get_llm_response(\n",
    "        system_prompt=system_prompt, \n",
    "        docs=json_docs, \n",
    "        query=query,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤ API\n",
    "    time.sleep(10)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>rag_answer</th>\n",
       "      <th>rag_answer_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some of the features of the yellow pa...</td>\n",
       "      <td>The yellow paper plates mentioned in the conte...</td>\n",
       "      <td>The features of the yellow paper plates mentio...</td>\n",
       "      <td>The features of the yellow paper plates mentio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a \"Cultural Muslim\" according to Kaigh...</td>\n",
       "      <td>A \"Cultural Muslim\" is someone who calls thems...</td>\n",
       "      <td>According to Kaighla Um Dayo, a!Cultural Musli...</td>\n",
       "      <td>According to Kaighla Um Dayo, a \"Cultural Musl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the main characters in the book \"Odd a...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "      <td>The main characters in the book \"Odd and the F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some features of the WHIRLPOOL BATHTU...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has t...</td>\n",
       "      <td>The WHIRLPOOL BATHTUB model AM152JDTS-1Z has t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the rank of Iasi in terms of populatio...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "      <td>Iasi is the 4th biggest city in Romania in ter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are some of the features of the yellow pa...   \n",
       "1  What is a \"Cultural Muslim\" according to Kaigh...   \n",
       "2  Who are the main characters in the book \"Odd a...   \n",
       "3  What are some features of the WHIRLPOOL BATHTU...   \n",
       "4  What is the rank of Iasi in terms of populatio...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The yellow paper plates mentioned in the conte...   \n",
       "1  A \"Cultural Muslim\" is someone who calls thems...   \n",
       "2  The main characters in the book \"Odd and the F...   \n",
       "3  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has s...   \n",
       "4  Iasi is the 4th biggest city in Romania in ter...   \n",
       "\n",
       "                                          rag_answer  \\\n",
       "0  The features of the yellow paper plates mentio...   \n",
       "1  According to Kaighla Um Dayo, a!Cultural Musli...   \n",
       "2  The main characters in the book \"Odd and the F...   \n",
       "3  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has t...   \n",
       "4  Iasi is the 4th biggest city in Romania in ter...   \n",
       "\n",
       "                                       rag_answer_v2  \n",
       "0  The features of the yellow paper plates mentio...  \n",
       "1  According to Kaighla Um Dayo, a \"Cultural Musl...  \n",
       "2  The main characters in the book \"Odd and the F...  \n",
       "3  The WHIRLPOOL BATHTUB model AM152JDTS-1Z has t...  \n",
       "4  Iasi is the 4th biggest city in Romania in ter...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['rag_answer_v2'] = df_sample['question'].apply(rag_answer_v2)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore F1 (–±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏): 0.9309\n"
     ]
    }
   ],
   "source": [
    "avg_f1_v2 = compute_bertscore(df_sample['rag_answer_v2'].tolist(), df_sample['answer'].tolist())\n",
    "print(f\"BertScore F1 (–±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏): {avg_f1_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó–ü–æ–ª–Ω—ã–π –∫–æ–¥ (—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏, —Å re-ranking, —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—Ç–≤–µ—Ç–æ–≤) - —Å–º–æ—Ç—Ä–∏—Ç–µ –≤ –ø—Ä–æ–µ–∫—Ç–µ‚ùó\n",
    "\n",
    "üè∑Ô∏è –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ **—É–ª—É—á—à–µ–Ω–∏—è**—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–±—É—é—Ç: \n",
    "1. üöÄ –î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ **–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏**\n",
    "2. üìñ **–î–æ–º–µ–Ω–Ω—É—é –æ–±–ª–∞—Å—Ç—å** (–¥–ª—è –ø—Ä–µ–¥—ä—è–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –æ—á–∏—Å—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π LLM-–º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π sentence-–º–æ–¥–µ–ª–∏ –∏ —Ç.–¥.)\n",
    "3. üîÑ **–ù–∞–ª–∏—á–∏–µ Flow**(MLflow\\Airflow\\Kubeflow) –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "## üì£ –í—ã–≤–æ–¥—ã:\n",
    "\n",
    "1. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ —Ä–∞–±–æ—á–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞, —Å–ø–æ—Å–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö (—Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö, –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ).\n",
    "\n",
    "2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —à–∞–≥–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—á–µ—Ä–µ–∑ DOC_RETRIEVAL_PROMPT) –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ (—á—Ç–æ –¥–æ–ª–∂–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ –º–∞–ª–æ–π LLM-–º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ BertScore –∏ –Ω–∞ –±–æ–ª–µ–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö\\–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö)\n",
    "\n",
    "3. –í—ã–±–æ—Ä –º–µ–∂–¥—É –ø–æ–¥—Ö–æ–¥–∞–º–∏ (—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –±–µ–∑ –Ω–µ—ë) –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
